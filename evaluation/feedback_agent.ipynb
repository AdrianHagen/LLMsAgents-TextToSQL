{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/Users/yannikhahn/Code/LLMsAgents-TextToSQL/src/agents', '/Users/yannikhahn/Code/LLMsAgents-TextToSQL/src', '/Users/yannikhahn/Code/LLMsAgents-TextToSQL/evaluation/src', '/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python312.zip', '/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12', '/opt/homebrew/Cellar/python@3.12/3.12.6/Frameworks/Python.framework/Versions/3.12/lib/python3.12/lib-dynload', '', '/Users/yannikhahn/Library/Caches/pypoetry/virtualenvs/llmsagents-texttosql-JgGAmzxu-py3.12/lib/python3.12/site-packages']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "sys.path.insert(0, os.path.abspath('../src'))\n",
    "sys.path.insert(0, os.path.abspath('../src/agents'))\n",
    "print(sys.path)\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>db_id</th>\n",
       "      <th>SQL</th>\n",
       "      <th>query_toks</th>\n",
       "      <th>query_toks_no_value</th>\n",
       "      <th>question</th>\n",
       "      <th>question_toks</th>\n",
       "      <th>sql</th>\n",
       "      <th>question_id</th>\n",
       "      <th>execution_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>planet_1</td>\n",
       "      <td>SELECT POSITION FROM Employee WHERE Name  =  \"...</td>\n",
       "      <td>['SELECT', 'POSITION', 'FROM', 'Employee', 'WH...</td>\n",
       "      <td>['select', 'position', 'from', 'employee', 'wh...</td>\n",
       "      <td>What is the position of Amy Wong?</td>\n",
       "      <td>['What', 'is', 'the', 'position', 'of', 'Amy',...</td>\n",
       "      <td>{'from': {'table_units': [['table_unit', 0]], ...</td>\n",
       "      <td>1859</td>\n",
       "      <td>0.000005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>institution_sports</td>\n",
       "      <td>SELECT T1.Nickname FROM championship AS T1 JOI...</td>\n",
       "      <td>['SELECT', 'T1.Nickname', 'FROM', 'championshi...</td>\n",
       "      <td>['select', 't1', '.', 'nickname', 'from', 'cha...</td>\n",
       "      <td>What is the nickname of the institution with t...</td>\n",
       "      <td>['What', 'is', 'the', 'nickname', 'of', 'the',...</td>\n",
       "      <td>{'from': {'table_units': [['table_unit', 1], [...</td>\n",
       "      <td>1659</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cre_Students_Information_Systems</td>\n",
       "      <td>SELECT datetime_detention_start ,  datetime_de...</td>\n",
       "      <td>['SELECT', 'datetime_detention_start', ',', 'd...</td>\n",
       "      <td>['select', 'datetime_detention_start', ',', 'd...</td>\n",
       "      <td>Show the detention start time and end time of ...</td>\n",
       "      <td>['Show', 'the', 'detention', 'start', 'time', ...</td>\n",
       "      <td>{'from': {'table_units': [['table_unit', 13]],...</td>\n",
       "      <td>477</td>\n",
       "      <td>0.000013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>customers_and_orders</td>\n",
       "      <td>SELECT product_name FROM Products WHERE produc...</td>\n",
       "      <td>['SELECT', 'product_name', 'FROM', 'Products',...</td>\n",
       "      <td>['select', 'product_name', 'from', 'products',...</td>\n",
       "      <td>Show all hardware type products in ascending o...</td>\n",
       "      <td>['Show', 'all', 'hardware', 'type', 'products'...</td>\n",
       "      <td>{'from': {'table_units': [['table_unit', 0]], ...</td>\n",
       "      <td>259</td>\n",
       "      <td>0.000008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>warehouse_1</td>\n",
       "      <td>SELECT sum(T1.value) FROM boxes AS T1 JOIN war...</td>\n",
       "      <td>['SELECT', 'sum', '(', 'T1.value', ')', 'FROM'...</td>\n",
       "      <td>['select', 'sum', '(', 't1', '.', 'value', ')'...</td>\n",
       "      <td>What is the total value of boxes located in Ch...</td>\n",
       "      <td>['What', 'is', 'the', 'total', 'value', 'of', ...</td>\n",
       "      <td>{'from': {'table_units': [['table_unit', 1], [...</td>\n",
       "      <td>1708</td>\n",
       "      <td>0.000007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              db_id  \\\n",
       "0                          planet_1   \n",
       "1                institution_sports   \n",
       "2  cre_Students_Information_Systems   \n",
       "3              customers_and_orders   \n",
       "4                       warehouse_1   \n",
       "\n",
       "                                                 SQL  \\\n",
       "0  SELECT POSITION FROM Employee WHERE Name  =  \"...   \n",
       "1  SELECT T1.Nickname FROM championship AS T1 JOI...   \n",
       "2  SELECT datetime_detention_start ,  datetime_de...   \n",
       "3  SELECT product_name FROM Products WHERE produc...   \n",
       "4  SELECT sum(T1.value) FROM boxes AS T1 JOIN war...   \n",
       "\n",
       "                                          query_toks  \\\n",
       "0  ['SELECT', 'POSITION', 'FROM', 'Employee', 'WH...   \n",
       "1  ['SELECT', 'T1.Nickname', 'FROM', 'championshi...   \n",
       "2  ['SELECT', 'datetime_detention_start', ',', 'd...   \n",
       "3  ['SELECT', 'product_name', 'FROM', 'Products',...   \n",
       "4  ['SELECT', 'sum', '(', 'T1.value', ')', 'FROM'...   \n",
       "\n",
       "                                 query_toks_no_value  \\\n",
       "0  ['select', 'position', 'from', 'employee', 'wh...   \n",
       "1  ['select', 't1', '.', 'nickname', 'from', 'cha...   \n",
       "2  ['select', 'datetime_detention_start', ',', 'd...   \n",
       "3  ['select', 'product_name', 'from', 'products',...   \n",
       "4  ['select', 'sum', '(', 't1', '.', 'value', ')'...   \n",
       "\n",
       "                                            question  \\\n",
       "0                  What is the position of Amy Wong?   \n",
       "1  What is the nickname of the institution with t...   \n",
       "2  Show the detention start time and end time of ...   \n",
       "3  Show all hardware type products in ascending o...   \n",
       "4  What is the total value of boxes located in Ch...   \n",
       "\n",
       "                                       question_toks  \\\n",
       "0  ['What', 'is', 'the', 'position', 'of', 'Amy',...   \n",
       "1  ['What', 'is', 'the', 'nickname', 'of', 'the',...   \n",
       "2  ['Show', 'the', 'detention', 'start', 'time', ...   \n",
       "3  ['Show', 'all', 'hardware', 'type', 'products'...   \n",
       "4  ['What', 'is', 'the', 'total', 'value', 'of', ...   \n",
       "\n",
       "                                                 sql  question_id  \\\n",
       "0  {'from': {'table_units': [['table_unit', 0]], ...         1859   \n",
       "1  {'from': {'table_units': [['table_unit', 1], [...         1659   \n",
       "2  {'from': {'table_units': [['table_unit', 13]],...          477   \n",
       "3  {'from': {'table_units': [['table_unit', 0]], ...          259   \n",
       "4  {'from': {'table_units': [['table_unit', 1], [...         1708   \n",
       "\n",
       "   execution_time  \n",
       "0        0.000005  \n",
       "1        0.000008  \n",
       "2        0.000013  \n",
       "3        0.000008  \n",
       "4        0.000007  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# sample = pd.read_csv(\"../sample/dev.csv\")\n",
    "sample = pd.read_csv(\"../sample/spider.csv\")\n",
    "\n",
    "sample.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "db_id                  100\n",
       "SQL                    100\n",
       "query_toks             100\n",
       "query_toks_no_value    100\n",
       "question               100\n",
       "question_toks          100\n",
       "sql                    100\n",
       "question_id            100\n",
       "execution_time         100\n",
       "dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama.chat_models import ChatOllama\n",
    "from langchain_openai import ChatOpenAI\n",
    "from agents.feedback_agent import FeedbackAgent\n",
    "from prompt_templates.feedback_agent import ONE_SHOT, TWO_SHOT\n",
    "\n",
    "model_name = \"mistral\"\n",
    "dataset_name = \"spider\"\n",
    "# llm = ChatOllama(model=\"mistral\")\n",
    "#llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "agent = FeedbackAgent(template=TWO_SHOT)\n",
    "model_type = \"two_shot\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354981f4c34d4344b4cbe4068cbb0429",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Evaluating mistral on spider:  90%|######### | 90/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: False\n",
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: True\n",
      "Model Correct: True\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import os\n",
    "\n",
    "os.makedirs(f\"./runs/feedback_agent/{dataset_name}/{model_name}\", exist_ok=True)\n",
    "\n",
    "try:\n",
    "    ongoing_run = pd.read_csv(\n",
    "        f\"./runs/feedback_agent/{dataset_name}/{model_name}/{model_type}.csv\"\n",
    "    ).to_dict(orient=\"records\")\n",
    "    evaluation = ongoing_run\n",
    "    start = len(ongoing_run)\n",
    "except FileNotFoundError:\n",
    "    evaluation = []\n",
    "    start = 0\n",
    "\n",
    "for row in tqdm(\n",
    "    sample[start : len(sample)].itertuples(index=False),\n",
    "    total=len(sample),\n",
    "    initial=start,\n",
    "    desc=f\"Evaluating {model_name} on {dataset_name}: \",\n",
    "):\n",
    "    response = agent._evaluate_query(\n",
    "        database=row.db_id, original_question=row.question, generated_sql_query=row.SQL\n",
    "    )\n",
    "    if response:\n",
    "        if dataset_name == \"bird\":\n",
    "            print(f\"Difficulty: {row.difficulty} | Model Correct: {response[\"is_correct\"]}\")\n",
    "        else:\n",
    "            print(f\"Model Correct: {response['is_correct']}\")\n",
    "        is_correct = response[\"is_correct\"]\n",
    "        successful_run = response[\"query_result\"] != \"error\"\n",
    "        feedback = response[\"feedback\"]\n",
    "    else:\n",
    "        is_correct = False\n",
    "        successful_run = False\n",
    "        feedback = \"LLM Failure\"\n",
    "\n",
    "    if dataset_name == \"bird\":\n",
    "        evaluation.append({\n",
    "            \"question_id\": row.question_id,\n",
    "            \"is_correct\": is_correct,\n",
    "            \"difficulty\": row.difficulty,\n",
    "            \"successful_run\": successful_run,\n",
    "            \"feedback\": feedback,\n",
    "        })\n",
    "    else:\n",
    "        evaluation.append(\n",
    "            {\n",
    "                \"question_id\": row.question_id,\n",
    "                \"is_correct\": is_correct,\n",
    "                \"successful_run\": successful_run,\n",
    "                \"feedback\": feedback,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(evaluation)\n",
    "    df.to_csv(\n",
    "        f\"./runs/feedback_agent/{dataset_name}/{model_name}/{model_type}.csv\",\n",
    "        index=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 72.00%\n"
     ]
    }
   ],
   "source": [
    "accuracy = df[\"is_correct\"].mean()\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "if dataset_name == \"bird\":\n",
    "    # Ensure all indices align with sorted difficulties\n",
    "    difficulty_order = sorted(df[\"difficulty\"].unique())\n",
    "\n",
    "    # Calculate metrics, ensuring alignment\n",
    "    accuracy_per_difficulty = (\n",
    "        df.groupby(\"difficulty\")[\"is_correct\"]\n",
    "        .mean()\n",
    "        .reindex(difficulty_order, fill_value=0)\n",
    "    )\n",
    "    successful_examples = (\n",
    "        df[df[\"successful_run\"] == True][\"difficulty\"]\n",
    "        .value_counts()\n",
    "        .reindex(difficulty_order, fill_value=0)\n",
    "    )\n",
    "    unsuccessful_examples = (\n",
    "        df[df[\"successful_run\"] == False][\"difficulty\"]\n",
    "        .value_counts()\n",
    "        .reindex(difficulty_order, fill_value=0)\n",
    "    )\n",
    "\n",
    "    # Calculate total examples for consistency (or use successful + unsuccessful)\n",
    "    examples_per_difficulty = successful_examples + unsuccessful_examples\n",
    "\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))  # Adjust figure size if needed\n",
    "\n",
    "    # Plot accuracy per difficulty\n",
    "    color = \"tab:blue\"\n",
    "    ax1.set_xlabel(\"Difficulty\")\n",
    "    ax1.set_ylabel(\"Accuracy\", color=color)\n",
    "    ax1.bar(\n",
    "        difficulty_order,\n",
    "        accuracy_per_difficulty,\n",
    "        color=color,\n",
    "        label=\"Accuracy\",\n",
    "        width=0.3,\n",
    "        align=\"center\",\n",
    "    )\n",
    "    ax1.tick_params(axis=\"y\", labelcolor=color)\n",
    "\n",
    "    # Create a secondary y-axis to plot examples\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel(\"Number of Examples\")\n",
    "\n",
    "    # Plot examples and unsuccessful runs per difficulty\n",
    "    width = 0.3  # Adjust bar width\n",
    "    positions = range(len(difficulty_order))  # Shared x-axis positions\n",
    "\n",
    "    # Stacked bar: successful and unsuccessful runs\n",
    "    ax2.bar(\n",
    "        [p + width for p in positions],\n",
    "        successful_examples,\n",
    "        color=\"tab:green\",\n",
    "        label=\"Successful Examples\",\n",
    "        width=width,\n",
    "        align=\"center\",\n",
    "    )\n",
    "    ax2.bar(\n",
    "        [p + width for p in positions],\n",
    "        unsuccessful_examples,\n",
    "        bottom=successful_examples,\n",
    "        color=\"tab:red\",\n",
    "        label=\"Unsuccessful Examples\",\n",
    "        width=width,\n",
    "        align=\"center\",\n",
    "    )\n",
    "\n",
    "    # Customize tick positions and labels\n",
    "    ax1.set_xticks([p + width for p in positions])\n",
    "    ax1.set_xticklabels(difficulty_order)\n",
    "\n",
    "    ax1.grid(False)\n",
    "    ax2.grid(False)\n",
    "\n",
    "    # Adjust legend placement\n",
    "    fig.legend(\n",
    "        loc=\"upper left\",\n",
    "        bbox_to_anchor=(0.2, 0.9),\n",
    "        bbox_transform=ax1.transAxes,\n",
    "        frameon=False,\n",
    "    )\n",
    "\n",
    "    # Add title\n",
    "    plt.title(\"Accuracy, Number of Examples, and Unsuccessful Runs per Difficulty\")\n",
    "\n",
    "    # Save the figure\n",
    "    fig.savefig(f\"runs/feedback_agent/{model_name}/{model_type}.png\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Support</th>\n",
       "      <th>Performance Gain</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Prompt</th>\n",
       "      <th>two_shot</th>\n",
       "      <th>two_shot</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mistral</th>\n",
       "      <td>0.72</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Accuracy  Support Performance Gain\n",
       "Prompt  two_shot two_shot                 \n",
       "Model                                     \n",
       "mistral     0.72    100.0              0.0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define the base directory\n",
    "base_dir = f\"./runs/feedback_agent/{dataset_name}\"\n",
    "\n",
    "# Initialize an empty dictionary to store the data\n",
    "data = {}\n",
    "\n",
    "# Iterate through each model directory\n",
    "for model in os.listdir(base_dir):\n",
    "    model_dir = os.path.join(base_dir, model)\n",
    "    if os.path.isdir(model_dir):\n",
    "        data[model] = {}\n",
    "        # Iterate through each CSV file in the model directory\n",
    "        for csv_file in os.listdir(model_dir):\n",
    "            if csv_file.endswith(\".csv\"):\n",
    "                csv_path = os.path.join(model_dir, csv_file)\n",
    "                prompt = os.path.splitext(csv_file)[0]\n",
    "                # Read the CSV file into a DataFrame\n",
    "                df = pd.read_csv(csv_path)\n",
    "                data[model][prompt] = df\n",
    "\n",
    "# Concatenate the DataFrames along a new axis\n",
    "concat_data = {(model, prompt): df for model, prompts in data.items() for prompt, df in prompts.items()}\n",
    "\n",
    "# Calculate accuracy and support for each model/prompt combination\n",
    "accuracies = {key: df[\"is_correct\"].mean() for key, df in concat_data.items()}\n",
    "supports = {key: df[\"successful_run\"].sum() for key, df in concat_data.items()}\n",
    "\n",
    "# Convert the accuracies and supports dictionaries to DataFrames for better visualization\n",
    "accuracy_df = pd.DataFrame(\n",
    "    list(accuracies.items()), columns=[\"Model_Prompt\", \"Accuracy\"]\n",
    ")\n",
    "support_df = pd.DataFrame(list(supports.items()), columns=[\"Model_Prompt\", \"Support\"])\n",
    "\n",
    "# Merge the accuracy and support DataFrames\n",
    "merged_df = pd.merge(accuracy_df, support_df, on=\"Model_Prompt\")\n",
    "\n",
    "# Split the Model_Prompt column into separate Model and Prompt columns\n",
    "merged_df[[\"Model\", \"Prompt\"]] = pd.DataFrame(\n",
    "    merged_df[\"Model_Prompt\"].tolist(), index=merged_df.index\n",
    ")\n",
    "merged_df.drop(columns=[\"Model_Prompt\"], inplace=True)\n",
    "\n",
    "# Pivot the DataFrame to have models as rows, prompts as columns, and accuracy as values\n",
    "pivot_df = merged_df.pivot(\n",
    "    index=\"Model\", columns=\"Prompt\", values=[\"Accuracy\", \"Support\"]\n",
    ")\n",
    "\n",
    "# Calculate the performance gain from worst to best\n",
    "worst_accuracy = pivot_df[\"Accuracy\"].min().min()\n",
    "pivot_df[\"Performance Gain\"] = (pivot_df[\"Accuracy\"].max(axis=1) - worst_accuracy) * 100\n",
    "\n",
    "pivot_df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmsagents-texttosql-JgGAmzxu-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
